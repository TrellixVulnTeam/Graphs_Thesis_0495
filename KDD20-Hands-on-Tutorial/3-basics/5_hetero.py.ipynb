{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import urllib.request\n",
    "\n",
    "# data_url = 'https://data.dgl.ai/dataset/ACM.mat'\n",
    "# data_file_path = '/tmp/ACM.mat'\n",
    "\n",
    "# urllib.request.urlretrieve(data_url, data_file_path)\n",
    "# data = scipy.io.loadmat(data_file_path)\n",
    "# print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseB = pd.read_csv('/home/harsh/Downloads/Thesis/Graph/KDD20-Hands-on-Tutorial/3-basics/data/houseB.csv')\n",
    "\n",
    "sensorTime = pd.read_csv('/home/harsh/Downloads/Thesis/Graph/KDD20-Hands-on-Tutorial/3-basics/data/houseB-sensorChangeTime.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sensor Timings CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "length_of_sensor_columns = len(nodesColumns_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_value_of_sensor_row_list = np.zeros(length_of_sensor_columns, dtype=int)\n",
    "for i in range(1,len(houseB)):        \n",
    "    for j in range(length_of_sensor_columns):        \n",
    "        prev_sensor_value = houseB.iloc[prev_value_of_sensor_row_list[j], 4+j]\n",
    "        current_sensor_value = houseB.iloc[i, 4+j]\n",
    "\n",
    "        prev_sensor_time_of_day = houseB.loc[prev_value_of_sensor_row_list[j], 'time_of_the_day']        \n",
    "        curr_sensor_time_of_day = houseB.loc[i, 'time_of_the_day']\n",
    "        diff = (curr_sensor_time_of_day - prev_sensor_time_of_day) * 24    \n",
    "        sensorTime.iloc[i, 4 + j] =  diff\n",
    "        \n",
    "        if prev_sensor_value != current_sensor_value:       \n",
    "            prev_value_of_sensor_row_list[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensorTime.to_csv('/home/harsh/Downloads/Thesis/Graph/Our_Graphs/houseB-sensorChangeTime.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'brushTeeth', 'id': 0}, {'name': 'eatBreakfast', 'id': 1}, {'name': 'eatDinner', 'id': 2}, {'name': 'getDressed', 'id': 3}, {'name': 'getDrink', 'id': 4}, {'name': 'goToBed', 'id': 5}, {'name': 'idle', 'id': 6}, {'name': 'leaveHouse', 'id': 7}, {'name': 'prepareBreakfast', 'id': 8}, {'name': 'prepareDinner', 'id': 9}, {'name': 'takeShower', 'id': 10}, {'name': 'useToilet', 'id': 11}, {'name': 'washDishes', 'id': 12}]\n"
     ]
    }
   ],
   "source": [
    "activity_list = []\n",
    "\n",
    "for i, val in enumerate(np.unique(houseB['activity'].values)):\n",
    "    d = {}\n",
    "    d['name'] = val\n",
    "    d['id'] = i\n",
    "    activity_list.append(d)\n",
    "    \n",
    "print(activity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>time_of_the_day</th>\n",
       "      <th>ToiletDoor_1</th>\n",
       "      <th>Fridge_3</th>\n",
       "      <th>GroceriesCupboard_5</th>\n",
       "      <th>ToiletFlush_6</th>\n",
       "      <th>Frontdoor_7</th>\n",
       "      <th>PlatesCupboard_9</th>\n",
       "      <th>BedroomDoor_10</th>\n",
       "      <th>...</th>\n",
       "      <th>BedroomDresser_18</th>\n",
       "      <th>BathroomPIR_19</th>\n",
       "      <th>PianoSeatPressure_20</th>\n",
       "      <th>SinkFloat_21</th>\n",
       "      <th>ServerNookSeatPressure_22</th>\n",
       "      <th>BalconyDoor_24</th>\n",
       "      <th>KitchenWindow_25</th>\n",
       "      <th>Toaster_26</th>\n",
       "      <th>Microwave_27</th>\n",
       "      <th>KitchenPIR_28</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brushTeeth</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eatBreakfast</th>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eatDinner</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getDressed</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getDrink</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prepareBreakfast</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prepareDinner</th>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>takeShower</th>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useToilet</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washDishes</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  start  end  time_of_the_day  ToiletDoor_1  Fridge_3  \\\n",
       "activity                                                                \n",
       "brushTeeth           35   35               35            35        35   \n",
       "eatBreakfast        136  136              136           136       136   \n",
       "eatDinner            49   49               49            49        49   \n",
       "getDressed           37   37               37            37        37   \n",
       "getDrink             14   14               14            14        14   \n",
       "prepareBreakfast     85   85               85            85        85   \n",
       "prepareDinner        89   89               89            89        89   \n",
       "takeShower          110  110              110           110       110   \n",
       "useToilet            64   64               64            64        64   \n",
       "washDishes           31   31               31            31        31   \n",
       "\n",
       "                  GroceriesCupboard_5  ToiletFlush_6  Frontdoor_7  \\\n",
       "activity                                                            \n",
       "brushTeeth                         35             35           35   \n",
       "eatBreakfast                      136            136          136   \n",
       "eatDinner                          49             49           49   \n",
       "getDressed                         37             37           37   \n",
       "getDrink                           14             14           14   \n",
       "prepareBreakfast                   85             85           85   \n",
       "prepareDinner                      89             89           89   \n",
       "takeShower                        110            110          110   \n",
       "useToilet                          64             64           64   \n",
       "washDishes                         31             31           31   \n",
       "\n",
       "                  PlatesCupboard_9  BedroomDoor_10  ...  BedroomDresser_18  \\\n",
       "activity                                            ...                      \n",
       "brushTeeth                      35              35  ...                 35   \n",
       "eatBreakfast                   136             136  ...                136   \n",
       "eatDinner                       49              49  ...                 49   \n",
       "getDressed                      37              37  ...                 37   \n",
       "getDrink                        14              14  ...                 14   \n",
       "prepareBreakfast                85              85  ...                 85   \n",
       "prepareDinner                   89              89  ...                 89   \n",
       "takeShower                     110             110  ...                110   \n",
       "useToilet                       64              64  ...                 64   \n",
       "washDishes                      31              31  ...                 31   \n",
       "\n",
       "                  BathroomPIR_19  PianoSeatPressure_20  SinkFloat_21  \\\n",
       "activity                                                               \n",
       "brushTeeth                    35                    35            35   \n",
       "eatBreakfast                 136                   136           136   \n",
       "eatDinner                     49                    49            49   \n",
       "getDressed                    37                    37            37   \n",
       "getDrink                      14                    14            14   \n",
       "prepareBreakfast              85                    85            85   \n",
       "prepareDinner                 89                    89            89   \n",
       "takeShower                   110                   110           110   \n",
       "useToilet                     64                    64            64   \n",
       "washDishes                    31                    31            31   \n",
       "\n",
       "                  ServerNookSeatPressure_22  BalconyDoor_24  KitchenWindow_25  \\\n",
       "activity                                                                        \n",
       "brushTeeth                               35              35                35   \n",
       "eatBreakfast                            136             136               136   \n",
       "eatDinner                                49              49                49   \n",
       "getDressed                               37              37                37   \n",
       "getDrink                                 14              14                14   \n",
       "prepareBreakfast                         85              85                85   \n",
       "prepareDinner                            89              89                89   \n",
       "takeShower                              110             110               110   \n",
       "useToilet                                64              64                64   \n",
       "washDishes                               31              31                31   \n",
       "\n",
       "                  Toaster_26  Microwave_27  KitchenPIR_28  \n",
       "activity                                                   \n",
       "brushTeeth                35            35             35  \n",
       "eatBreakfast             136           136            136  \n",
       "eatDinner                 49            49             49  \n",
       "getDressed                37            37             37  \n",
       "getDrink                  14            14             14  \n",
       "prepareBreakfast          85            85             85  \n",
       "prepareDinner             89            89             89  \n",
       "takeShower               110           110            110  \n",
       "useToilet                 64            64             64  \n",
       "washDishes                31            31             31  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houseB = houseB[houseB['activity']!='goToBed']\n",
    "houseB = houseB[houseB['activity']!='leaveHouse']\n",
    "houseB = houseB[houseB['activity']!='idle']\n",
    "houseB.groupby('activity').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv('/home/harsh/Downloads/Thesis/Graph/Our_Graphs/nodes.csv')\n",
    "\n",
    "edges = pd.read_csv('/home/harsh/Downloads/Thesis/Graph/Our_Graphs/bidrectional_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = edges['Src']\n",
    "v = edges['Dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    " config = {\n",
    "     \"ActivityIdList\" :\n",
    "   [{'name': 'brushTeeth', 'id': 0}, {'name': 'eatBreakfast', 'id': 1}, {'name': 'eatDinner', 'id': 2}, {'name': 'getDressed', 'id': 3}, {'name': 'getDrink', 'id': 4}, {'name': 'goToBed', 'id': 5}, {'name': 'idle', 'id': 6}, {'name': 'leaveHouse', 'id': 7}, {'name': 'prepareBreakfast', 'id': 8}, {'name': 'prepareDinner', 'id': 9}, {'name': 'takeShower', 'id': 10}, {'name': 'useToilet', 'id': 11}, {'name': 'washDishes', 'id': 12}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDFromClassName(train_label):\n",
    "    ActivityIdList = config['ActivityIdList']\n",
    "    train_label = [x for x in ActivityIdList if x[\"name\"] == train_label]\n",
    "    return train_label[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph per row of the House CSV\n",
    "graphs = []\n",
    "labels = []\n",
    "\n",
    "# Combine Feature like this: Place_in_House,Type, Value for each node\n",
    "for i in range(len(houseB)):\n",
    "# for i in range(5000):\n",
    "    \n",
    "    feature  = []\n",
    "    \n",
    "    # Define Graph\n",
    "    g = dgl.graph((u, v))\n",
    "\n",
    "    # Add Features\n",
    "    for j in range(len(nodes)):\n",
    "        try:\n",
    "            node_value = houseB.iloc[i, 4+j]        \n",
    "        except:\n",
    "            node_value = -1  \n",
    "                            \n",
    "        node_place_in_house = nodes.loc[j, 'place_in_house']\n",
    "        node_type = nodes.loc[j, 'Type']\n",
    "        feature.append([node_value, node_place_in_house, node_type])    \n",
    "    g.ndata['feat'] = torch.tensor(feature)\n",
    "    \n",
    "    # Give Label\n",
    "    labels.append(getIDFromClassName(houseB.iloc[i, 2]))\n",
    "    graphs.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class GraphHouseDataset():    \n",
    "    def __init__(self, graphs, labels):\n",
    "        super(GraphHouseDataset, self).__init__()\n",
    "        self.graphs = graphs\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get graph and label by index\"\"\"     \n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of graphs in the dataset\"\"\"\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Start for some other dataset to check network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.data\n",
    "dataset = dgl.data.GINDataset('MUTAG', False)\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    for g in graphs:\n",
    "        # deal with node feats\n",
    "        for key in g.node_attr_schemes().keys():\n",
    "            g.ndata[key] = g.ndata[key].float()\n",
    "        # no edge feats\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.tensor(labels)\n",
    "    return batched_graph, labels\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"End for some other dataset to check network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DataLoaders\n",
    "# dataset = GraphHouseDataset(graphs, labels)\n",
    "\n",
    "# # create collate_fn\n",
    "# def collate(samples):\n",
    "#     # The input `samples` is a list of pairs (graph, label).\n",
    "#     graphs, labels = map(list, zip(*samples))    \n",
    "#     batched_graph = dgl.batch(graphs)\n",
    "#     labels = torch.tensor(labels)\n",
    "#     return batched_graph, labels\n",
    "\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [node_value, node_place_in_house, node_type]\n",
    "torch.manual_seed(0)\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_dim, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # Apply graph convolution and activation.\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "            return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss 445.6690, Train Acc 0.3936 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 202.6177, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 136.8479, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 130.7178, Train Acc 0.6862 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 131.1927, Train Acc 0.5851 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 130.7272, Train Acc 0.5798 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 130.1169, Train Acc 0.5798 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 129.5733, Train Acc 0.5798 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 129.1212, Train Acc 0.5798 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 128.7089, Train Acc 0.5851 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 128.3367, Train Acc 0.5904 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 127.9451, Train Acc 0.6064 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 127.5707, Train Acc 0.6277 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 127.1402, Train Acc 0.6436 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 126.7600, Train Acc 0.6436 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 126.3372, Train Acc 0.6489 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 126.0140, Train Acc 0.6596 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 125.5622, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 125.1036, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 124.7540, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 124.3603, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 123.9012, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 123.4082, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 122.9898, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 122.5969, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 122.1880, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 121.6851, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 121.1739, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 120.6813, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 120.2655, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 119.6848, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 119.1559, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 118.7411, Train Acc 0.6596 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 118.1255, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 117.6814, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 117.0370, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 116.6757, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 116.0532, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 115.7193, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 115.0958, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 126.5036, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 115.0279, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 114.8690, Train Acc 0.6489 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 113.7012, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 113.0867, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 112.5560, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 111.9542, Train Acc 0.6489 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 111.5686, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 111.1245, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 110.7089, Train Acc 0.6489 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 110.2617, Train Acc 0.6596 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 110.0864, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 109.6193, Train Acc 0.6596 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 109.4284, Train Acc 0.6596 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 108.9441, Train Acc 0.6596 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 115.2944, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 109.7485, Train Acc 0.6543 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 108.8881, Train Acc 0.6649 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 108.2695, Train Acc 0.6755 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 108.0922, Train Acc 0.6862 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 107.7108, Train Acc 0.6862 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 107.6148, Train Acc 0.6862 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 107.3393, Train Acc 0.6968 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 107.1748, Train Acc 0.6968 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.8963, Train Acc 0.6968 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.9202, Train Acc 0.6968 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.5892, Train Acc 0.7074 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.7986, Train Acc 0.7128 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.3793, Train Acc 0.7128 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.0776, Train Acc 0.7128 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.6391, Train Acc 0.7128 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.1396, Train Acc 0.7074 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.9852, Train Acc 0.7074 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.8524, Train Acc 0.7074 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.7387, Train Acc 0.7074 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.5341, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.5093, Train Acc 0.6968 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.4142, Train Acc 0.6968 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.4346, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.3200, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.3422, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.1543, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.9899, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.0382, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.9487, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.8619, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.7929, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.6336, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.5424, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.5424, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.7485, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.2921, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.4619, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 106.9523, Train Acc 0.7074 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 105.1211, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.5654, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.5367, Train Acc 0.7021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.3206, Train Acc 0.7128 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.2786, Train Acc 0.7128 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loss 104.0919, Train Acc 0.7128 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Featuresize, hiddenLayerSize, num_classes\n",
    "model = Classifier(7, 10, 13)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    acc= []\n",
    "    for i, (batched_graphs, labels) in enumerate(dataloader):\n",
    "        feats = batched_graphs.ndata['attr'].float()        \n",
    "        logits = model(batched_graphs, feats)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        pred = logits.argmax(1)  \n",
    "#         if i % 500 == 0:\n",
    "#             print(pred)\n",
    "        train_acc = (pred == labels).sum().item()/len(pred)\n",
    "        acc.append(train_acc)\n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    acc = np.array(acc).mean()    \n",
    "    print('\\n\\nLoss %.4f, Train Acc %.4f \\n\\n' % (\n",
    "        epoch_loss.item(),\n",
    "        acc.item(),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[10,20,30,40], [10,20,30,40], [10,20,30,40]])\n",
    "b = torch.tensor([10,20,31,41])\n",
    "\n",
    "print(a.argmax(1))\n",
    "# c = (a == b).sum().item()\n",
    "\n",
    "# c1 = len(a)\n",
    "\n",
    "# print(c/c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1666666666666665"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3.5]\n",
    "np.array(a, dtype=float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dgl.data\n",
    "# dataset = dgl.data.GINDataset('MUTAG', False)\n",
    "\n",
    "\n",
    "# dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate(samples):\n",
    "#     graphs, labels = map(list, zip(*samples))\n",
    "#     batched_graph = dgl.batch(graphs)\n",
    "#     batched_labels = torch.tensor(labels)\n",
    "#     return batched_graph, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=1024,\n",
    "#     collate_fn=collate,\n",
    "#     drop_last=False,\n",
    "#     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'connectedWith_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-20199f51670e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatched_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-20199f51670e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-20199f51670e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# inputs is features of nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/dgl/nn/pytorch/hetero.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, inputs, mod_args, mod_kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 dstdata = self.mods[etype](\n\u001b[0m\u001b[1;32m    171\u001b[0m                     \u001b[0mrel_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_copy_to_script_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'connectedWith_1'"
     ]
    }
   ],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs is features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "\n",
    "class HeteroClassifier(nn.Module):\n",
    "    def __init__(self, G, in_dim, hidden_dim, n_classes, rel_names):\n",
    "        super().__init__()                \n",
    "        \n",
    "        \n",
    "        self.rgcn = RGCN(in_dim, hidden_dim, hidden_dim, rel_names)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \n",
    "        h = g.ndata\n",
    "        h = self.rgcn(g, h)\n",
    "        with g.local_scope():\n",
    "            g.ndata = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = 0\n",
    "            for ntype in g.ntypes:\n",
    "                hg = hg + dgl.mean_nodes(g, 'h', ntype=ntype)\n",
    "            return self.classify(hg)\n",
    "        \n",
    "        \n",
    "model = HeteroClassifier(g, 5, 20, 5, 'connectedWith_1')\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(20):\n",
    "    for batched_graph, labels in dataloader:\n",
    "        logits = model(batched_graph)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'room': {'place_in_house': tensor([0, 1, 2, 3, 4])}, 'sensor': {'place_in_house': tensor([2, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 3, 2, 4, 3, 0, 0, 0, 0]), 'currentSensorvalues': tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0])}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class HeteroRGCNLayer(nn.Module):\n",
    "    def __init__(self, in_size, out_size, etypes):\n",
    "        super(HeteroRGCNLayer, self).__init__()\n",
    "        # W_r for each relation\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name : nn.Linear(in_size, out_size) for name in etypes\n",
    "            })\n",
    "        \n",
    "\n",
    "    def forward(self, G, feat_dict):\n",
    "        # The input is a dictionary of node features for each type\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # Compute W_r * h            \n",
    "            Wh = self.weight[etype](feat_dict[srctype])\n",
    "            print(feat_dict[srctype])\n",
    "            # Save it in graph for message passing\n",
    "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "            # Specify per-relation message passing functions: (message_func, reduce_func).\n",
    "            # Note that the results are saved to the same destination feature 'h', which\n",
    "            # hints the type wise reducer for aggregation.\n",
    "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        # Trigger message passing of multiple types.\n",
    "        # The first argument is the message passing functions for each relation.\n",
    "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
    "        # \"min\", \"mean\", \"stack\"\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        # return the updated node feature dictionary\n",
    "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}\n",
    "    \n",
    "    \n",
    "class HeteroRGCN(nn.Module):\n",
    "    def __init__(self, G, in_size, hidden_size, out_size):\n",
    "        super(HeteroRGCN, self).__init__()\n",
    "        # Use trainable node embeddings as featureless inputs.\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), in_size))\n",
    "                      for ntype in G.ntypes}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        # create layers\n",
    "        self.layer1 = HeteroRGCNLayer(in_size, hidden_size, G.etypes)\n",
    "        self.layer2 = HeteroRGCNLayer(hidden_size, out_size, G.etypes)\n",
    "\n",
    "    def forward(self, G):\n",
    "        h_dict = self.layer1(G, self.embed)\n",
    "        h_dict = {k : F.leaky_relu(h) for k, h in h_dict.items()}\n",
    "        h_dict = self.layer2(G, h_dict)\n",
    "        \n",
    "        # get paper logits\n",
    "        return h_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroRGCN(g, 5, 40, 20)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1982,  0.2756,  0.5585,  0.6418,  0.5529],\n",
      "        [-0.0388,  0.5257,  0.7347,  0.3640, -0.3275],\n",
      "        [ 0.4052, -0.3667, -0.4774,  0.3016, -0.2666],\n",
      "        [-0.2678,  0.0197,  0.4296,  0.3594, -0.1597],\n",
      "        [ 0.0518, -0.3857, -0.1902,  0.6816,  0.5124]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1982,  0.2756,  0.5585,  0.6418,  0.5529],\n",
      "        [-0.0388,  0.5257,  0.7347,  0.3640, -0.3275],\n",
      "        [ 0.4052, -0.3667, -0.4774,  0.3016, -0.2666],\n",
      "        [-0.2678,  0.0197,  0.4296,  0.3594, -0.1597],\n",
      "        [ 0.0518, -0.3857, -0.1902,  0.6816,  0.5124]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2559,  0.1799,  0.1891,  0.2021, -0.1346],\n",
      "        [ 0.1436,  0.0144,  0.4015, -0.2170, -0.4146],\n",
      "        [-0.4713, -0.1568, -0.1289,  0.1052, -0.3894],\n",
      "        [-0.3615,  0.4373,  0.2693, -0.0702, -0.1023],\n",
      "        [ 0.2586, -0.2345, -0.0489,  0.2224, -0.1748],\n",
      "        [-0.3641, -0.3023, -0.1429,  0.2922,  0.2682],\n",
      "        [ 0.2961,  0.1842, -0.4578, -0.2655,  0.3149],\n",
      "        [ 0.4191, -0.4101,  0.4035, -0.2862,  0.0740],\n",
      "        [-0.0937,  0.0564,  0.1787,  0.3301,  0.3672],\n",
      "        [ 0.1348,  0.4424,  0.4454, -0.0909, -0.0256],\n",
      "        [ 0.1125,  0.3964, -0.3498,  0.0067, -0.0018],\n",
      "        [-0.2117,  0.4452, -0.4402,  0.0872, -0.0089],\n",
      "        [ 0.0912, -0.2356, -0.1299, -0.3569,  0.3470],\n",
      "        [-0.3146,  0.1149, -0.0053,  0.1705,  0.4596],\n",
      "        [ 0.3096,  0.1013,  0.0470,  0.0442, -0.1906],\n",
      "        [-0.4586, -0.3992,  0.2364,  0.0850, -0.1194],\n",
      "        [ 0.0422,  0.0599, -0.4364, -0.1266,  0.1895],\n",
      "        [-0.4125, -0.1875, -0.0495,  0.0063,  0.3886],\n",
      "        [-0.0451,  0.2649,  0.1503,  0.0140, -0.2348],\n",
      "        [-0.0241,  0.2916, -0.1355, -0.1035,  0.4334],\n",
      "        [-0.4047, -0.2491, -0.1334, -0.4234, -0.2606],\n",
      "        [-0.3187,  0.2798,  0.4605, -0.2311, -0.2749]], requires_grad=True)\n",
      "tensor([[ 2.9954e-01, -3.5188e-04, -1.0786e-03, -4.8587e-03,  1.3282e-01,\n",
      "          3.9051e-02,  8.2874e-02, -2.3869e-03, -4.1536e-03, -9.5875e-04,\n",
      "         -4.7792e-03,  9.1516e-02, -6.3167e-04, -1.7072e-03, -5.3816e-03,\n",
      "          6.5407e-02,  2.1545e-01, -2.0533e-03,  2.2123e-03, -6.1705e-03,\n",
      "         -6.5948e-03,  3.0199e-01, -1.1988e-03, -1.8140e-03,  6.0380e-02,\n",
      "          4.7383e-01, -2.9645e-03, -6.5315e-06,  2.0957e-01, -5.5815e-03,\n",
      "         -4.2923e-04,  4.2463e-02, -7.9974e-03,  2.4725e-01, -6.9994e-04,\n",
      "         -3.4620e-03, -2.4944e-04,  2.9463e-01, -2.9010e-03,  7.0949e-02],\n",
      "        [ 1.3676e-01,  1.0647e-01, -1.6134e-03, -4.1232e-03, -2.5913e-04,\n",
      "          3.6954e-01,  1.4178e-01, -3.0542e-03, -3.4049e-03,  5.9765e-02,\n",
      "         -2.9432e-03, -1.6339e-03,  2.5288e-01, -6.7874e-04, -6.1640e-03,\n",
      "          6.8958e-03,  2.6893e-01, -3.8361e-03, -2.0710e-03, -4.4269e-03,\n",
      "         -7.4776e-03,  6.1630e-01, -1.6265e-04, -4.9572e-03,  1.8780e-01,\n",
      "          6.0208e-01, -5.4368e-03, -1.0748e-03, -1.0615e-04, -4.5631e-03,\n",
      "         -1.2562e-03,  4.5790e-01, -8.0231e-03,  5.5042e-01,  7.2232e-02,\n",
      "         -4.2284e-03, -3.4892e-03,  5.1710e-01, -3.3366e-03,  7.3987e-02],\n",
      "        [ 2.6627e-02,  7.6143e-02, -3.9257e-03, -5.1480e-03, -1.0486e-03,\n",
      "          7.2566e-01,  1.3504e-01,  6.3383e-02, -2.2679e-03,  4.0405e-02,\n",
      "          6.9064e-02, -2.2611e-03, -7.0268e-04, -3.7795e-03, -4.1996e-03,\n",
      "          1.9068e-01,  4.1589e-01, -3.9597e-03, -3.8994e-03, -4.0369e-03,\n",
      "         -9.2087e-03,  6.1461e-01, -6.8954e-04, -3.4756e-03,  3.1181e-01,\n",
      "          9.5382e-01, -7.2117e-03, -1.9410e-03, -1.0183e-03, -4.6581e-03,\n",
      "         -4.0877e-03,  4.9941e-01, -7.8535e-03,  7.1322e-01,  4.2754e-01,\n",
      "         -7.5884e-03, -1.8822e-03,  6.7503e-01, -4.7727e-03,  2.5693e-01],\n",
      "        [ 1.1338e-01,  1.4206e-01, -1.9741e-03, -3.7950e-03, -3.8785e-04,\n",
      "          4.3536e-01,  1.7066e-01, -1.5272e-03, -3.2769e-03,  4.8542e-02,\n",
      "         -1.9177e-03, -1.2193e-03,  1.2478e-01, -1.5411e-03, -5.2841e-03,\n",
      "          1.3384e-02,  3.0150e-01, -3.6676e-03, -2.3395e-03, -4.1014e-03,\n",
      "         -8.5226e-03,  5.7367e-01, -5.5392e-04, -3.5338e-03,  2.3146e-01,\n",
      "          6.4678e-01, -5.2182e-03, -6.5291e-04, -2.9411e-04, -4.4688e-03,\n",
      "         -2.0009e-03,  4.0522e-01, -8.2617e-03,  5.4817e-01,  1.9186e-01,\n",
      "         -5.9110e-03, -1.8859e-03,  5.3661e-01, -3.6939e-03,  1.4669e-01],\n",
      "        [ 4.0224e-01,  2.0073e-01,  3.2554e-02, -4.4993e-03, -2.7391e-04,\n",
      "          7.7296e-02, -2.9536e-04, -4.9851e-03, -4.9274e-03,  1.1451e-01,\n",
      "         -4.0762e-03, -5.3098e-04,  3.1190e-01,  8.1507e-02, -6.4136e-03,\n",
      "         -1.6586e-03, -4.1578e-05, -2.7310e-03, -1.1096e-03, -5.1743e-03,\n",
      "         -5.5724e-03,  5.2709e-01,  1.2246e-01, -4.6173e-03,  1.3398e-01,\n",
      "          2.7390e-01, -4.1331e-03, -1.0993e-03,  1.3199e-01, -3.1230e-03,\n",
      "         -3.2735e-04,  1.8894e-01, -5.9408e-03,  3.9295e-01, -1.4989e-04,\n",
      "         -9.1254e-04, -2.8921e-03,  3.2960e-01, -2.8687e-03, -1.6402e-03]],\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[ 2.9954e-01, -3.5188e-04, -1.0786e-03, -4.8587e-03,  1.3282e-01,\n",
      "          3.9051e-02,  8.2874e-02, -2.3869e-03, -4.1536e-03, -9.5875e-04,\n",
      "         -4.7792e-03,  9.1516e-02, -6.3167e-04, -1.7072e-03, -5.3816e-03,\n",
      "          6.5407e-02,  2.1545e-01, -2.0533e-03,  2.2123e-03, -6.1705e-03,\n",
      "         -6.5948e-03,  3.0199e-01, -1.1988e-03, -1.8140e-03,  6.0380e-02,\n",
      "          4.7383e-01, -2.9645e-03, -6.5315e-06,  2.0957e-01, -5.5815e-03,\n",
      "         -4.2923e-04,  4.2463e-02, -7.9974e-03,  2.4725e-01, -6.9994e-04,\n",
      "         -3.4620e-03, -2.4944e-04,  2.9463e-01, -2.9010e-03,  7.0949e-02],\n",
      "        [ 1.3676e-01,  1.0647e-01, -1.6134e-03, -4.1232e-03, -2.5913e-04,\n",
      "          3.6954e-01,  1.4178e-01, -3.0542e-03, -3.4049e-03,  5.9765e-02,\n",
      "         -2.9432e-03, -1.6339e-03,  2.5288e-01, -6.7874e-04, -6.1640e-03,\n",
      "          6.8958e-03,  2.6893e-01, -3.8361e-03, -2.0710e-03, -4.4269e-03,\n",
      "         -7.4776e-03,  6.1630e-01, -1.6265e-04, -4.9572e-03,  1.8780e-01,\n",
      "          6.0208e-01, -5.4368e-03, -1.0748e-03, -1.0615e-04, -4.5631e-03,\n",
      "         -1.2562e-03,  4.5790e-01, -8.0231e-03,  5.5042e-01,  7.2232e-02,\n",
      "         -4.2284e-03, -3.4892e-03,  5.1710e-01, -3.3366e-03,  7.3987e-02],\n",
      "        [ 2.6627e-02,  7.6143e-02, -3.9257e-03, -5.1480e-03, -1.0486e-03,\n",
      "          7.2566e-01,  1.3504e-01,  6.3383e-02, -2.2679e-03,  4.0405e-02,\n",
      "          6.9064e-02, -2.2611e-03, -7.0268e-04, -3.7795e-03, -4.1996e-03,\n",
      "          1.9068e-01,  4.1589e-01, -3.9597e-03, -3.8994e-03, -4.0369e-03,\n",
      "         -9.2087e-03,  6.1461e-01, -6.8954e-04, -3.4756e-03,  3.1181e-01,\n",
      "          9.5382e-01, -7.2117e-03, -1.9410e-03, -1.0183e-03, -4.6581e-03,\n",
      "         -4.0877e-03,  4.9941e-01, -7.8535e-03,  7.1322e-01,  4.2754e-01,\n",
      "         -7.5884e-03, -1.8822e-03,  6.7503e-01, -4.7727e-03,  2.5693e-01],\n",
      "        [ 1.1338e-01,  1.4206e-01, -1.9741e-03, -3.7950e-03, -3.8785e-04,\n",
      "          4.3536e-01,  1.7066e-01, -1.5272e-03, -3.2769e-03,  4.8542e-02,\n",
      "         -1.9177e-03, -1.2193e-03,  1.2478e-01, -1.5411e-03, -5.2841e-03,\n",
      "          1.3384e-02,  3.0150e-01, -3.6676e-03, -2.3395e-03, -4.1014e-03,\n",
      "         -8.5226e-03,  5.7367e-01, -5.5392e-04, -3.5338e-03,  2.3146e-01,\n",
      "          6.4678e-01, -5.2182e-03, -6.5291e-04, -2.9411e-04, -4.4688e-03,\n",
      "         -2.0009e-03,  4.0522e-01, -8.2617e-03,  5.4817e-01,  1.9186e-01,\n",
      "         -5.9110e-03, -1.8859e-03,  5.3661e-01, -3.6939e-03,  1.4669e-01],\n",
      "        [ 4.0224e-01,  2.0073e-01,  3.2554e-02, -4.4993e-03, -2.7391e-04,\n",
      "          7.7296e-02, -2.9536e-04, -4.9851e-03, -4.9274e-03,  1.1451e-01,\n",
      "         -4.0762e-03, -5.3098e-04,  3.1190e-01,  8.1507e-02, -6.4136e-03,\n",
      "         -1.6586e-03, -4.1578e-05, -2.7310e-03, -1.1096e-03, -5.1743e-03,\n",
      "         -5.5724e-03,  5.2709e-01,  1.2246e-01, -4.6173e-03,  1.3398e-01,\n",
      "          2.7390e-01, -4.1331e-03, -1.0993e-03,  1.3199e-01, -3.1230e-03,\n",
      "         -3.2735e-04,  1.8894e-01, -5.9408e-03,  3.9295e-01, -1.4989e-04,\n",
      "         -9.1254e-04, -2.8921e-03,  3.2960e-01, -2.8687e-03, -1.6402e-03]],\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[-3.8376e-03, -9.9185e-04,  4.0254e-01, -1.0776e-03,  3.1282e-01,\n",
      "         -3.0133e-03, -6.5742e-03,  5.5857e-01, -1.3430e-03,  4.1314e-02,\n",
      "         -1.7758e-03,  2.7331e-01, -1.1226e-03, -2.8661e-03, -6.4673e-04,\n",
      "          8.2897e-02, -7.5634e-04,  2.7129e-01,  1.2988e-01, -1.5161e-04,\n",
      "          2.6213e-01, -9.2107e-05, -8.9707e-04,  4.1498e-01,  3.2810e-01,\n",
      "          2.9902e-01,  3.8642e-01, -5.0660e-04, -1.8013e-03, -1.0842e-03,\n",
      "         -3.9164e-03, -1.8030e-03, -3.5091e-03, -1.9224e-03, -5.9423e-05,\n",
      "          9.1608e-02,  6.9959e-02,  4.0483e-01, -2.0622e-03, -1.7453e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [-3.8376e-03, -9.9185e-04,  4.0254e-01, -1.0776e-03,  3.1282e-01,\n",
      "         -3.0133e-03, -6.5742e-03,  5.5857e-01, -1.3430e-03,  4.1314e-02,\n",
      "         -1.7758e-03,  2.7331e-01, -1.1226e-03, -2.8661e-03, -6.4673e-04,\n",
      "          8.2897e-02, -7.5634e-04,  2.7129e-01,  1.2988e-01, -1.5161e-04,\n",
      "          2.6213e-01, -9.2107e-05, -8.9707e-04,  4.1498e-01,  3.2810e-01,\n",
      "          2.9902e-01,  3.8642e-01, -5.0660e-04, -1.8013e-03, -1.0842e-03,\n",
      "         -3.9164e-03, -1.8030e-03, -3.5091e-03, -1.9224e-03, -5.9423e-05,\n",
      "          9.1608e-02,  6.9959e-02,  4.0483e-01, -2.0622e-03, -1.7453e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 9.9089e-02, -1.0415e-03,  1.0402e-01, -2.8316e-03,  2.7604e-01,\n",
      "         -4.3111e-03, -3.7373e-03, -9.3802e-05, -2.9976e-03,  1.7208e-01,\n",
      "         -2.3284e-03,  6.1763e-01, -8.3665e-04, -3.2350e-03,  5.9390e-01,\n",
      "          2.6265e-01,  4.8205e-02, -3.6856e-03, -1.0409e-03,  4.5176e-02,\n",
      "          7.1918e-01,  1.5981e-01, -2.2123e-03,  8.2296e-02,  6.7413e-02,\n",
      "          8.3670e-01,  1.1014e-01,  1.9367e-01,  3.5617e-01,  9.1107e-02,\n",
      "         -7.9107e-05, -6.4487e-06,  2.9050e-01, -6.6260e-03,  5.5118e-01,\n",
      "         -3.0144e-03,  7.5151e-02,  4.9523e-01, -2.2473e-03, -2.5942e-03],\n",
      "        [ 9.9089e-02, -1.0415e-03,  1.0402e-01, -2.8316e-03,  2.7604e-01,\n",
      "         -4.3111e-03, -3.7373e-03, -9.3802e-05, -2.9976e-03,  1.7208e-01,\n",
      "         -2.3284e-03,  6.1763e-01, -8.3665e-04, -3.2350e-03,  5.9390e-01,\n",
      "          2.6265e-01,  4.8205e-02, -3.6856e-03, -1.0409e-03,  4.5176e-02,\n",
      "          7.1918e-01,  1.5981e-01, -2.2123e-03,  8.2296e-02,  6.7413e-02,\n",
      "          8.3670e-01,  1.1014e-01,  1.9367e-01,  3.5617e-01,  9.1107e-02,\n",
      "         -7.9107e-05, -6.4487e-06,  2.9050e-01, -6.6260e-03,  5.5118e-01,\n",
      "         -3.0144e-03,  7.5151e-02,  4.9523e-01, -2.2473e-03, -2.5942e-03],\n",
      "        [ 9.9089e-02, -1.0415e-03,  1.0402e-01, -2.8316e-03,  2.7604e-01,\n",
      "         -4.3111e-03, -3.7373e-03, -9.3802e-05, -2.9976e-03,  1.7208e-01,\n",
      "         -2.3284e-03,  6.1763e-01, -8.3665e-04, -3.2350e-03,  5.9390e-01,\n",
      "          2.6265e-01,  4.8205e-02, -3.6856e-03, -1.0409e-03,  4.5176e-02,\n",
      "          7.1918e-01,  1.5981e-01, -2.2123e-03,  8.2296e-02,  6.7413e-02,\n",
      "          8.3670e-01,  1.1014e-01,  1.9367e-01,  3.5617e-01,  9.1107e-02,\n",
      "         -7.9107e-05, -6.4487e-06,  2.9050e-01, -6.6260e-03,  5.5118e-01,\n",
      "         -3.0144e-03,  7.5151e-02,  4.9523e-01, -2.2473e-03, -2.5942e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 9.9089e-02, -1.0415e-03,  1.0402e-01, -2.8316e-03,  2.7604e-01,\n",
      "         -4.3111e-03, -3.7373e-03, -9.3802e-05, -2.9976e-03,  1.7208e-01,\n",
      "         -2.3284e-03,  6.1763e-01, -8.3665e-04, -3.2350e-03,  5.9390e-01,\n",
      "          2.6265e-01,  4.8205e-02, -3.6856e-03, -1.0409e-03,  4.5176e-02,\n",
      "          7.1918e-01,  1.5981e-01, -2.2123e-03,  8.2296e-02,  6.7413e-02,\n",
      "          8.3670e-01,  1.1014e-01,  1.9367e-01,  3.5617e-01,  9.1107e-02,\n",
      "         -7.9107e-05, -6.4487e-06,  2.9050e-01, -6.6260e-03,  5.5118e-01,\n",
      "         -3.0144e-03,  7.5151e-02,  4.9523e-01, -2.2473e-03, -2.5942e-03],\n",
      "        [ 9.9089e-02, -1.0415e-03,  1.0402e-01, -2.8316e-03,  2.7604e-01,\n",
      "         -4.3111e-03, -3.7373e-03, -9.3802e-05, -2.9976e-03,  1.7208e-01,\n",
      "         -2.3284e-03,  6.1763e-01, -8.3665e-04, -3.2350e-03,  5.9390e-01,\n",
      "          2.6265e-01,  4.8205e-02, -3.6856e-03, -1.0409e-03,  4.5176e-02,\n",
      "          7.1918e-01,  1.5981e-01, -2.2123e-03,  8.2296e-02,  6.7413e-02,\n",
      "          8.3670e-01,  1.1014e-01,  1.9367e-01,  3.5617e-01,  9.1107e-02,\n",
      "         -7.9107e-05, -6.4487e-06,  2.9050e-01, -6.6260e-03,  5.5118e-01,\n",
      "         -3.0144e-03,  7.5151e-02,  4.9523e-01, -2.2473e-03, -2.5942e-03],\n",
      "        [-3.8376e-03, -9.9185e-04,  4.0254e-01, -1.0776e-03,  3.1282e-01,\n",
      "         -3.0133e-03, -6.5742e-03,  5.5857e-01, -1.3430e-03,  4.1314e-02,\n",
      "         -1.7758e-03,  2.7331e-01, -1.1226e-03, -2.8661e-03, -6.4673e-04,\n",
      "          8.2897e-02, -7.5634e-04,  2.7129e-01,  1.2988e-01, -1.5161e-04,\n",
      "          2.6213e-01, -9.2107e-05, -8.9707e-04,  4.1498e-01,  3.2810e-01,\n",
      "          2.9902e-01,  3.8642e-01, -5.0660e-04, -1.8013e-03, -1.0842e-03,\n",
      "         -3.9164e-03, -1.8030e-03, -3.5091e-03, -1.9224e-03, -5.9423e-05,\n",
      "          9.1608e-02,  6.9959e-02,  4.0483e-01, -2.0622e-03, -1.7453e-03],\n",
      "        [-3.1228e-04, -1.9259e-03,  3.1789e-02, -2.7934e-03,  1.6966e-01,\n",
      "         -2.8487e-03, -1.9352e-03,  2.8815e-01, -3.7635e-03,  1.3395e-01,\n",
      "         -3.7085e-03,  3.9223e-01, -1.5823e-04, -2.9537e-03,  4.0647e-01,\n",
      "          2.8772e-01, -1.2357e-03, -2.7039e-03, -1.3753e-03,  4.2188e-02,\n",
      "          4.4565e-01, -2.0379e-04, -5.1018e-04,  2.7007e-01,  5.6303e-02,\n",
      "          6.1704e-01,  3.2574e-01,  3.1141e-01,  1.6411e-01,  1.6712e-01,\n",
      "          1.2447e-01,  6.0091e-02,  5.6761e-02, -4.1519e-03,  4.6048e-01,\n",
      "          1.2893e-02,  5.0048e-02,  3.7911e-01, -4.6417e-03, -2.4110e-03],\n",
      "        [-3.8376e-03, -9.9185e-04,  4.0254e-01, -1.0776e-03,  3.1282e-01,\n",
      "         -3.0133e-03, -6.5742e-03,  5.5857e-01, -1.3430e-03,  4.1314e-02,\n",
      "         -1.7758e-03,  2.7331e-01, -1.1226e-03, -2.8661e-03, -6.4673e-04,\n",
      "          8.2897e-02, -7.5634e-04,  2.7129e-01,  1.2988e-01, -1.5161e-04,\n",
      "          2.6213e-01, -9.2107e-05, -8.9707e-04,  4.1498e-01,  3.2810e-01,\n",
      "          2.9902e-01,  3.8642e-01, -5.0660e-04, -1.8013e-03, -1.0842e-03,\n",
      "         -3.9164e-03, -1.8030e-03, -3.5091e-03, -1.9224e-03, -5.9423e-05,\n",
      "          9.1608e-02,  6.9959e-02,  4.0483e-01, -2.0622e-03, -1.7453e-03],\n",
      "        [-6.3308e-04, -1.4016e-03,  3.3370e-01,  6.7462e-02,  4.3983e-02,\n",
      "         -2.6698e-03, -1.0735e-03,  1.6443e-01, -1.7605e-03, -8.0107e-04,\n",
      "         -1.8122e-03, -6.4159e-04, -9.4050e-05, -4.2970e-03,  2.3548e-01,\n",
      "          2.7797e-01, -1.8683e-03, -2.3401e-03,  2.2439e-02,  4.9387e-02,\n",
      "          3.6633e-01, -7.3913e-04,  1.3689e-01,  3.7762e-01,  3.5738e-01,\n",
      "          2.6239e-01,  4.7153e-01,  2.5573e-01,  6.3400e-02,  4.5663e-01,\n",
      "         -1.5793e-03, -4.2334e-04, -1.1225e-03,  9.7014e-02,  2.1717e-01,\n",
      "         -1.0379e-03,  1.1941e-01, -2.5092e-04, -7.4647e-03, -1.7293e-03],\n",
      "        [-3.1228e-04, -1.9259e-03,  3.1789e-02, -2.7934e-03,  1.6966e-01,\n",
      "         -2.8487e-03, -1.9352e-03,  2.8815e-01, -3.7635e-03,  1.3395e-01,\n",
      "         -3.7085e-03,  3.9223e-01, -1.5823e-04, -2.9537e-03,  4.0647e-01,\n",
      "          2.8772e-01, -1.2357e-03, -2.7039e-03, -1.3753e-03,  4.2188e-02,\n",
      "          4.4565e-01, -2.0379e-04, -5.1018e-04,  2.7007e-01,  5.6303e-02,\n",
      "          6.1704e-01,  3.2574e-01,  3.1141e-01,  1.6411e-01,  1.6712e-01,\n",
      "          1.2447e-01,  6.0091e-02,  5.6761e-02, -4.1519e-03,  4.6048e-01,\n",
      "          1.2893e-02,  5.0048e-02,  3.7911e-01, -4.6417e-03, -2.4110e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03],\n",
      "        [ 2.7293e-01,  3.7731e-03,  2.8492e-01,  9.5769e-02,  1.3067e-01,\n",
      "         -3.4894e-03, -5.5411e-04, -3.6901e-03, -1.4431e-03, -1.9297e-03,\n",
      "         -1.1267e-04,  1.5587e-01, -8.4241e-04, -4.6701e-03,  6.9178e-01,\n",
      "          3.7048e-01,  3.5653e-02, -5.9394e-03,  1.6168e-02,  1.3624e-01,\n",
      "          7.2970e-01,  2.3133e-01,  1.8046e-02, -1.3934e-05,  3.5538e-01,\n",
      "          5.6163e-01,  1.8675e-01,  3.2617e-01,  5.0692e-01,  4.9657e-01,\n",
      "         -1.4914e-03,  3.5991e-02,  2.5190e-01, -2.9987e-03,  4.7373e-01,\n",
      "         -5.4858e-03,  2.2175e-01,  8.3986e-02, -6.1804e-03, -2.8857e-03]],\n",
      "       grad_fn=<LeakyReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    for g, labels in dataloader:    \n",
    "        logits = model(g)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['connectedWith', 'connectedWith', 'connectedWith']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.etypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/harsh/Downloads/Thesis/Research/data/houseB/houseB.json') as f:\n",
    "#     jsonFile = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id =-1 \n",
    "# for dictionary in jsonFile['sensorLocation']:\n",
    "#     id+= 1\n",
    "#     print(id,',', dictionary['name'],',', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'TvsP', 'PvsA', 'PvsV', 'AvsF', 'VvsC', 'PvsL', 'PvsC', 'A', 'C', 'F', 'L', 'P', 'T', 'V', 'PvsT', 'CNormPvsA', 'RNormPvsA', 'CNormPvsC', 'RNormPvsC', 'CNormPvsT', 'RNormPvsT', 'CNormPvsV', 'RNormPvsV', 'CNormVvsC', 'RNormVvsC', 'CNormAvsF', 'RNormAvsF', 'CNormPvsL', 'RNormPvsL', 'stopwords', 'nPvsT', 'nT', 'CNormnPvsT', 'RNormnPvsT', 'nnPvsT', 'nnT', 'CNormnnPvsT', 'RNormnnPvsT', 'PvsP', 'CNormPvsP', 'RNormPvsP'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csc.csc_matrix'>\n",
      "#Papers: 12499\n",
      "#Authors: 17431\n",
      "#Links: 37055\n"
     ]
    }
   ],
   "source": [
    "print(type(data['PvsA']))\n",
    "print('#Papers:', data['PvsA'].shape[0])\n",
    "print('#Authors:', data['PvsA'].shape[1])\n",
    "print('#Links:', data['PvsA'].nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4610)\t1.0\n",
      "  (0, 5298)\t1.0\n",
      "  (0, 12982)\t1.0\n"
     ]
    }
   ],
   "source": [
    "x = data['PvsA'][0, :]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_g = dgl.heterograph({('paper', 'written-by', 'author') : data['PvsA'].nonzero()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types: ['author', 'paper']\n",
      "Edge types: ['written-by']\n",
      "Canonical edge types: [('paper', 'written-by', 'author')]\n"
     ]
    }
   ],
   "source": [
    "print('Node types:', pa_g.ntypes)\n",
    "print('Edge types:', pa_g.etypes)\n",
    "print('Canonical edge types:', pa_g.canonical_etypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12499\n",
      "37055\n",
      "37055\n",
      "tensor([3532, 6421, 8516, 8560])\n",
      "37055\n"
     ]
    }
   ],
   "source": [
    "print(pa_g.number_of_nodes('paper'))\n",
    "# Canonical edge type name can be shortened to only one edge type name if it is\n",
    "# uniquely distinguishable.\n",
    "print(pa_g.number_of_edges(('paper', 'written-by', 'author')))\n",
    "print(pa_g.number_of_edges('written-by'))\n",
    "print(pa_g.successors(1, etype='written-by'))  # get the authors that write paper #1\n",
    "print(pa_g.number_of_edges())  # Only one edge type, the edge type argument could be omitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12499\n",
      "30789\n",
      "tensor([1361, 2624, 8670, 9845])\n"
     ]
    }
   ],
   "source": [
    "pp_g = dgl.heterograph({('paper', 'citing', 'paper') : data['PvsP'].nonzero()})\n",
    "# equivalent (shorter) API for creating homogeneous graph\n",
    "pp_g = dgl.from_scipy(data['PvsP'])\n",
    "\n",
    "# All the ntype and etype arguments could be omitted because the behavior is unambiguous.\n",
    "print(pp_g.number_of_nodes())\n",
    "print(pp_g.number_of_edges())\n",
    "print(pp_g.successors(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x17431 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['PvsA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.heterograph({\n",
    "        ('paper', 'written-by', 'author') : data['PvsA'].nonzero(),\n",
    "        ('author', 'writing', 'paper') : data['PvsA'].transpose().nonzero(),\n",
    "        ('paper', 'citing', 'paper') : data['PvsP'].nonzero(),\n",
    "        ('paper', 'cited', 'paper') : data['PvsP'].transpose().nonzero(),\n",
    "        ('paper', 'is-about', 'subject') : data['PvsL'].nonzero(),\n",
    "        ('subject', 'has', 'paper') : data['PvsL'].transpose().nonzero(),\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the metagraph using graphviz.\n",
    "import pygraphviz as pgv\n",
    "def plot_graph(nxg):\n",
    "    ag = pgv.AGraph(strict=False, directed=True)\n",
    "    for u, v, k in nxg.edges(keys=True):\n",
    "        ag.add_edge(u, v, label=k)\n",
    "    ag.layout('dot')\n",
    "    ag.draw('graph.png')\n",
    "\n",
    "plot_graph(G.metagraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pvc = data['PvsC'].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all papers published in KDD, ICML, VLDB\n",
    "c_selected = [0, 11, 13]  # KDD, ICML, VLDB\n",
    "p_selected = pvc[:, c_selected].tocoo()\n",
    "# generate labels\n",
    "labels = pvc.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels == 11] = 1\n",
    "labels[labels == 13] = 2\n",
    "labels = torch.tensor(labels).long()\n",
    "\n",
    "# generate train/val/test split\n",
    "pid = p_selected.row\n",
    "shuffle = np.random.permutation(pid)\n",
    "train_idx = torch.tensor(shuffle[0:800]).long()\n",
    "val_idx = torch.tensor(shuffle[800:900]).long()\n",
    "test_idx = torch.tensor(shuffle[900:]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ec83e9aaaa7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHeteroRGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class HeteroRGCNLayer(nn.Module):\n",
    "    def __init__(self, in_size, out_size, etypes):\n",
    "        super(HeteroRGCNLayer, self).__init__()\n",
    "        # W_r for each relation\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name : nn.Linear(in_size, out_size) for name in etypes\n",
    "            })\n",
    "\n",
    "    def forward(self, G, feat_dict):\n",
    "        # The input is a dictionary of node features for each type\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # Compute W_r * h\n",
    "            print(self.weight[etype])\n",
    "            Wh = self.weight[etype](feat_dict[srctype])\n",
    "            # Save it in graph for message passing\n",
    "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "            # Specify per-relation message passing functions: (message_func, reduce_func).\n",
    "            # Note that the results are saved to the same destination feature 'h', which\n",
    "            # hints the type wise reducer for aggregation.\n",
    "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        # Trigger message passing of multiple types.\n",
    "        # The first argument is the message passing functions for each relation.\n",
    "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
    "        # \"min\", \"mean\", \"stack\"\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        # return the updated node feature dictionary\n",
    "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}\n",
    "    \n",
    "    \n",
    "    \n",
    "class HeteroRGCN(nn.Module):\n",
    "    def __init__(self, G, in_size, hidden_size, out_size):\n",
    "        super(HeteroRGCN, self).__init__()\n",
    "        # Use trainable node embeddings as featureless inputs.\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), in_size))\n",
    "                      for ntype in G.ntypes}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        # create layers\n",
    "        self.layer1 = HeteroRGCNLayer(in_size, hidden_size, G.etypes)\n",
    "        self.layer2 = HeteroRGCNLayer(hidden_size, out_size, G.etypes)\n",
    "\n",
    "    def forward(self, G):\n",
    "        h_dict = self.layer1(G, self.embed)\n",
    "        h_dict = {k : F.leaky_relu(h) for k, h in h_dict.items()}\n",
    "        h_dict = self.layer2(G, h_dict)\n",
    "        # get paper logits\n",
    "        return h_dict['paper']\n",
    "    \n",
    "    \n",
    "    \n",
    "model = HeteroRGCN(G, 10, 40, 20)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'writing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-321-be21dea5547d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# The loss is computed only for labeled nodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-320-ab826cd9df4a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, G)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mh_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mh_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mh_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-320-ab826cd9df4a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, G, feat_dict)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msrctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsttype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_etypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Compute W_r * h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mWh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrctype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Save it in graph for message passing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrctype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wh_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_copy_to_script_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'writing'"
     ]
    }
   ],
   "source": [
    "\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    logits = model(G)\n",
    "    # The loss is computed only for labeled nodes.\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "\n",
    "    pred = logits.argmax(1)\n",
    "    train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "    val_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "    test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "\n",
    "    if best_val_acc < val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Loss %.4f, Train Acc %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (\n",
    "            loss.item(),\n",
    "            train_acc.item(),\n",
    "            val_acc.item(),\n",
    "            best_val_acc.item(),\n",
    "            test_acc.item(),\n",
    "            best_test_acc.item(),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
